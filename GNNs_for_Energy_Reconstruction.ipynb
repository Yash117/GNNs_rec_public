{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GNNs for Energy Reconstruction.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "kDqzaHHiiP2N",
        "JRyXyzPniJva",
        "MNvDu7yVU5ym",
        "dcKubELoi22Z",
        "ZhZhYZAnBKi-",
        "TAW-m6BGFUXS",
        "guQtj33kFoEf",
        "OuAUH2-Mr6XD",
        "0f4j8_7DHUEK",
        "QgMirSEOHjEi",
        "LLouho2OItyb",
        "G0D2Gxb8o6FX",
        "2KgBPvX1o_kn",
        "eN-IeHM-TmN4",
        "rn1bxW39sb71",
        "XMR9pmSfXFnC",
        "kdQ2URoZk8Lk",
        "QI-oq0TAuVxL"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89YNFG73vksc"
      },
      "source": [
        "# Energy Reconstruction using Graph Regression in the Spektral library of python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDqzaHHiiP2N"
      },
      "source": [
        "# Importing libraries:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qX_3i6ShF3iJ",
        "outputId": "49f27e36-a045-4427-8e6b-5f1e7aed6819"
      },
      "source": [
        "%%time\n",
        "import numpy as np\n",
        "import itertools\n",
        "import os\n",
        "import shutil;\n",
        "import random\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "import subprocess\n",
        "import pandas as pd\n",
        "import scipy \n",
        "from scipy.stats import norm\n",
        "from matplotlib.patches import Rectangle\n",
        "import scipy\n",
        "from scipy.stats import norm\n",
        "from keras.callbacks import EarlyStopping\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 2.03 s, sys: 369 ms, total: 2.4 s\n",
            "Wall time: 2.74 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TBl_20PgGBKq"
      },
      "source": [
        "!pip install spektral"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNmn5g_F4Rjo"
      },
      "source": [
        "from spektral.data import DisjointLoader\n",
        "from spektral.datasets import QM9\n",
        "from spektral.layers import ECCConv, GlobalSumPool "
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4uMdi8rCsUW"
      },
      "source": [
        "from spektral import datasets"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f27zdfjfA9H5"
      },
      "source": [
        "import scipy.sparse as sp\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.losses import MeanAbsoluteError\n",
        "from tensorflow.keras.metrics import categorical_accuracy\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "from spektral.data import Dataset, DisjointLoader, Graph\n",
        "from spektral.layers import GCSConv, GlobalAvgPool\n",
        "from spektral.layers.pooling import TopKPool\n",
        "from spektral.transforms.normalize_adj import NormalizeAdj"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JRyXyzPniJva"
      },
      "source": [
        "# Loading data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zp1tRjemtzBH"
      },
      "source": [
        "DF_training_input = pd.DataFrame(np.load('/content/drive/MyDrive/SKKU_JSNS^2/v6_250k_samples_JSNS^2/training_input.npy'))\n",
        "DF_training_output = pd.DataFrame(np.load('/content/drive/MyDrive/SKKU_JSNS^2/v6_250k_samples_JSNS^2/training_output.npy'))\n",
        "DF_test_input = pd.DataFrame(np.load('/content/drive/MyDrive/SKKU_JSNS^2/v6_250k_samples_JSNS^2/test_input.npy'))\n",
        "DF_test_output = pd.DataFrame(np.load('/content/drive/MyDrive/SKKU_JSNS^2/v6_250k_samples_JSNS^2/test_output.npy')) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QS_Qt2w85eq2"
      },
      "source": [
        "DF_training_input = DF_training_input/(500)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "grDYmdKHKWEg"
      },
      "source": [
        "for i in range(len(DF_training_input)):\n",
        "  for j in range(96):\n",
        "    if DF_training_input[j][i] > 1.0:\n",
        "      DF_training_input[j][i] = 1.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YkqgNYzuL0qu"
      },
      "source": [
        "DF_test_input = DF_test_input/(500)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MqiIPjPOMb7M"
      },
      "source": [
        "for i in range(len(DF_test_input)):\n",
        "  for j in range(96):\n",
        "    if DF_test_input[j][i] > 1.0:\n",
        "      DF_test_input[j][i] = 1.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNvDu7yVU5ym"
      },
      "source": [
        "# Extracting coordinates:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sa81tONLRWlB"
      },
      "source": [
        "d_coordinates = pd.read_csv('/content/drive/MyDrive/SKKU_JSNS^2/PMT_coordinates.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcKubELoi22Z"
      },
      "source": [
        "# Defining some variables:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jl0umZQ-jA-U"
      },
      "source": [
        "learning_rate = 1e-2  # Learning rate\n",
        "epochs = 10  # Number of training epochs\n",
        "es_patience = 10  # Patience for early stopping\n",
        "batch_size = 32  # Batch size\n",
        "max_no_of_graphs_training = DF_training_input.shape[0] \n",
        "max_no_of_graphs_test = DF_test_input.shape[0] "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZhZhYZAnBKi-"
      },
      "source": [
        "# Creating dataset (function):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TAW-m6BGFUXS"
      },
      "source": [
        "##Creating the adjacency matrix (remains same for all graphs, only node features change):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3HMekqc2DP9Y"
      },
      "source": [
        "adj = np.zeros((96,96)) \n",
        "\n",
        "for k1 in range(96): \n",
        "  for k2 in range(96): \n",
        "    dist = ((d_coordinates['x'][k1] - d_coordinates['x'][k2])**2 + (d_coordinates['y'][k1] - d_coordinates['y'][k2])**2 + (d_coordinates['z'][k1] - d_coordinates['z'][k2])**2)**0.5 \n",
        "    inv_sq = 0 \n",
        "    if dist != 0: \n",
        "      inv_sq = 1 / (dist**2) \n",
        "    adj[k1][k2] = inv_sq"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yX6p1phljmar"
      },
      "source": [
        "Max-min normalization:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sTMq0I3hFhfi"
      },
      "source": [
        "min_adj = adj.min()\n",
        "max_adj = adj.max()\n",
        "for i in range(96):\n",
        "  for j in range(96):\n",
        "    adj[i][j] = (adj[i][j] - min_adj) / (max_adj - min_adj)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3QmswYoejhCo"
      },
      "source": [
        "Making the diagonal elements = 1:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EyCrjuYtFbZO"
      },
      "source": [
        "for i in range(96):\n",
        "  for j in range(96):\n",
        "    if i == j:\n",
        "      adj[i][j] = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVqxHM3Gjyfi"
      },
      "source": [
        "Adjacency matrix is symmetric:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uDEkkz3DFbXM",
        "outputId": "4d5a904d-cd23-4b13-af40-3e074bcf0437"
      },
      "source": [
        "adj == adj.T "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ True,  True,  True, ...,  True,  True,  True],\n",
              "       [ True,  True,  True, ...,  True,  True,  True],\n",
              "       [ True,  True,  True, ...,  True,  True,  True],\n",
              "       ...,\n",
              "       [ True,  True,  True, ...,  True,  True,  True],\n",
              "       [ True,  True,  True, ...,  True,  True,  True],\n",
              "       [ True,  True,  True, ...,  True,  True,  True]])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "guQtj33kFoEf"
      },
      "source": [
        "##Classes to create datasets:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bY0_rCTkvC9"
      },
      "source": [
        "Training dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48LseNb1BGqK"
      },
      "source": [
        "class MyDataset(Dataset):\n",
        "    def __init__(self, n_samples, **kwargs): \n",
        "        self.n_samples = n_samples \n",
        "        \n",
        "        super().__init__(**kwargs) \n",
        "\n",
        "    def read(self): \n",
        "      all_graphs = [] \n",
        "      for i in range(len(DF_training_output[:max_no_of_graphs_training])): \n",
        "        # Node features \n",
        "        x = np.array(DF_training_input.loc[i]).reshape(96,1) \n",
        "\n",
        "        # Edges \n",
        "        a = adj \n",
        "\n",
        "        # Labels \n",
        "        y = np.zeros(1) \n",
        "        y[0] = DF_training_output[0][i] \n",
        "\n",
        "        all_graphs.append(Graph(x=x, a=a, y=y)) \n",
        "\n",
        "      # We must return a list of Graph objects \n",
        "      return [all_graphs[i] for i in range(self.n_samples)] "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6tD5JNmblCVB"
      },
      "source": [
        "Test dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nVr8zCUQLDPc"
      },
      "source": [
        "class MyTestDataset(Dataset):\n",
        "    def __init__(self, n_samples, **kwargs):\n",
        "        self.n_samples = n_samples\n",
        "        \n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "    def read(self):\n",
        "      all_graphs = []\n",
        "      for i in range(len(DF_test_output[:max_no_of_graphs_test])): \n",
        "        # Node features \n",
        "        x = np.array(DF_test_input.loc[i]).reshape(96,1) \n",
        "\n",
        "        # Edges \n",
        "        a = adj \n",
        "\n",
        "        # Labels \n",
        "        y = np.zeros(1) \n",
        "        y[0] = DF_test_output[0][i] \n",
        "\n",
        "        all_graphs.append(Graph(x=x, a=a, y=y)) \n",
        "\n",
        "      # We must return a list of Graph objects \n",
        "      return [all_graphs[i] for i in range(self.n_samples)] "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1qDY8XipdNvA"
      },
      "source": [
        "Mono energetic data imports:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_PFXJCQVdrdD"
      },
      "source": [
        "D_mono_input1 = pd.read_csv('/content/drive/MyDrive/SKKU_JSNS^2/Mono-energetic_clean_data/1MeV_input.csv')\n",
        "D_mono_output1 = pd.read_csv('/content/drive/MyDrive/SKKU_JSNS^2/Mono-energetic_clean_data/1MeV_output.csv')\n",
        "D_mono_output1.columns = [0] \n",
        "#cleaning the mono-energetic data \n",
        "D_mono_input1 = D_mono_input1/(500) \n",
        "D_mono_input1.columns = np.arange(96) \n",
        "for k1 in range(len(D_mono_input1)): \n",
        "  for k2 in range(96): \n",
        "    if D_mono_input1[k2][k1] > 1.0: \n",
        "      D_mono_input1[k2][k1] = 1.0 \n",
        "\n",
        "\n",
        "D_mono_input5 = pd.read_csv('/content/drive/MyDrive/SKKU_JSNS^2/Mono-energetic_clean_data/5MeV_input.csv')\n",
        "D_mono_output5 = pd.read_csv('/content/drive/MyDrive/SKKU_JSNS^2/Mono-energetic_clean_data/5MeV_output.csv')\n",
        "D_mono_output5.columns = [0] \n",
        "#cleaning the mono-energetic data \n",
        "D_mono_input5 = D_mono_input5/(500) \n",
        "D_mono_input5.columns = np.arange(96) \n",
        "for k1 in range(len(D_mono_input5)): \n",
        "  for k2 in range(96): \n",
        "    if D_mono_input5[k2][k1] > 1.0: \n",
        "      D_mono_input5[k2][k1] = 1.0 \n",
        "\n",
        "\n",
        "D_mono_input8 = pd.read_csv('/content/drive/MyDrive/SKKU_JSNS^2/Mono-energetic_clean_data/8MeV_input.csv')\n",
        "D_mono_output8 = pd.read_csv('/content/drive/MyDrive/SKKU_JSNS^2/Mono-energetic_clean_data/8MeV_output.csv')\n",
        "D_mono_output8.columns = [0] \n",
        "#cleaning the mono-energetic data \n",
        "D_mono_input8 = D_mono_input8/(500) \n",
        "D_mono_input8.columns = np.arange(96) \n",
        "for k1 in range(len(D_mono_input8)): \n",
        "  for k2 in range(96): \n",
        "    if D_mono_input8[k2][k1] > 1.0: \n",
        "      D_mono_input8[k2][k1] = 1.0 \n",
        "\n",
        "\n",
        "D_mono_input10 = pd.read_csv('/content/drive/MyDrive/SKKU_JSNS^2/Mono-energetic_clean_data/10MeV_input.csv')\n",
        "D_mono_output10 = pd.read_csv('/content/drive/MyDrive/SKKU_JSNS^2/Mono-energetic_clean_data/10MeV_output.csv')\n",
        "D_mono_output10.columns = [0] \n",
        "#cleaning the mono-energetic data \n",
        "D_mono_input10 = D_mono_input10/(500) \n",
        "D_mono_input10.columns = np.arange(96) \n",
        "for k1 in range(len(D_mono_input10)): \n",
        "  for k2 in range(96): \n",
        "    if D_mono_input10[k2][k1] > 1.0: \n",
        "      D_mono_input10[k2][k1] = 1.0 \n",
        "\n",
        "\n",
        "D_mono_input20 = pd.read_csv('/content/drive/MyDrive/SKKU_JSNS^2/Mono-energetic_clean_data/20MeV_input.csv')\n",
        "D_mono_output20 = pd.read_csv('/content/drive/MyDrive/SKKU_JSNS^2/Mono-energetic_clean_data/20MeV_output.csv')\n",
        "D_mono_output20.columns = [0] \n",
        "#cleaning the mono-energetic data \n",
        "D_mono_input20 = D_mono_input20/(500) \n",
        "D_mono_input20.columns = np.arange(96) \n",
        "for k1 in range(len(D_mono_input20)): \n",
        "  for k2 in range(96): \n",
        "    if D_mono_input20[k2][k1] > 1.0: \n",
        "      D_mono_input20[k2][k1] = 1.0 \n",
        "\n",
        "\n",
        "D_mono_input30 = pd.read_csv('/content/drive/MyDrive/SKKU_JSNS^2/Mono-energetic_clean_data/30MeV_input.csv')\n",
        "D_mono_output30 = pd.read_csv('/content/drive/MyDrive/SKKU_JSNS^2/Mono-energetic_clean_data/30MeV_output.csv')\n",
        "D_mono_output30.columns = [0] \n",
        "#cleaning the mono-energetic data \n",
        "D_mono_input30 = D_mono_input30/(500) \n",
        "D_mono_input30.columns = np.arange(96) \n",
        "for k1 in range(len(D_mono_input30)): \n",
        "  for k2 in range(96): \n",
        "    if D_mono_input30[k2][k1] > 1.0: \n",
        "      D_mono_input30[k2][k1] = 1.0 \n",
        "\n",
        "\n",
        "D_mono_input40 = pd.read_csv('/content/drive/MyDrive/SKKU_JSNS^2/Mono-energetic_clean_data/40MeV_input.csv')\n",
        "D_mono_output40 = pd.read_csv('/content/drive/MyDrive/SKKU_JSNS^2/Mono-energetic_clean_data/40MeV_output.csv')\n",
        "D_mono_output40.columns = [0] \n",
        "#cleaning the mono-energetic data \n",
        "D_mono_input40 = D_mono_input40/(500) \n",
        "D_mono_input40.columns = np.arange(96) \n",
        "for k1 in range(len(D_mono_input40)): \n",
        "  for k2 in range(96): \n",
        "    if D_mono_input40[k2][k1] > 1.0: \n",
        "      D_mono_input40[k2][k1] = 1.0 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "POFMMMWolXp8"
      },
      "source": [
        "Mono-energetic dataset classes:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UcwrQ8_0lfVR"
      },
      "source": [
        "1 MeV:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Co49GEUxcmm9"
      },
      "source": [
        "class MyMonoEnergeticDataset1(Dataset):\n",
        "\n",
        "    def __init__(self, n_samples, **kwargs):\n",
        "        self.n_samples = n_samples\n",
        "        \n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "    def read(self):\n",
        "      all_graphs = []\n",
        "      for i in range(len(D_mono_output1)): \n",
        "        # Node features \n",
        "        x = np.array(D_mono_input1.loc[i]).reshape(96,1) \n",
        "\n",
        "        # Edges \n",
        "        a = adj \n",
        "\n",
        "        # Labels \n",
        "        y = np.zeros(1) \n",
        "        y[0] = D_mono_output1[0][i] \n",
        "\n",
        "        all_graphs.append(Graph(x=x, a=a, y=y)) \n",
        "\n",
        "      # We must return a list of Graph objects \n",
        "      return [all_graphs[i] for i in range(self.n_samples)] "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROjs8EymljiB"
      },
      "source": [
        "5 MeV:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DThm2JlbdIuY"
      },
      "source": [
        "class MyMonoEnergeticDataset5(Dataset):\n",
        "  def __init__(self, n_samples, **kwargs):\n",
        "    self.n_samples = n_samples\n",
        "    \n",
        "    super().__init__(**kwargs)\n",
        "\n",
        "  def read(self):\n",
        "    all_graphs = []\n",
        "    for i in range(len(D_mono_output5)): \n",
        "      # Node features \n",
        "      x = np.array(D_mono_input5.loc[i]).reshape(96,1) \n",
        "\n",
        "      # Edges \n",
        "      a = adj \n",
        "\n",
        "      # Labels \n",
        "      y = np.zeros(1) \n",
        "      y[0] = D_mono_output5[0][i] \n",
        "\n",
        "      all_graphs.append(Graph(x=x, a=a, y=y)) \n",
        "\n",
        "    # We must return a list of Graph objects \n",
        "    return [all_graphs[i] for i in range(self.n_samples)] "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PuzD-loZllp1"
      },
      "source": [
        "8 MeV:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gMUTOXSLdIqI"
      },
      "source": [
        "class MyMonoEnergeticDataset8(Dataset):\n",
        "  def __init__(self, n_samples, **kwargs):\n",
        "    self.n_samples = n_samples\n",
        "    \n",
        "    super().__init__(**kwargs)\n",
        "\n",
        "  def read(self):\n",
        "    all_graphs = []\n",
        "    for i in range(len(D_mono_output8)): \n",
        "      # Node features \n",
        "      x = np.array(D_mono_input8.loc[i]).reshape(96,1) \n",
        "\n",
        "      # Edges \n",
        "      a = adj \n",
        "\n",
        "      # Labels \n",
        "      y = np.zeros(1) \n",
        "      y[0] = D_mono_output8[0][i] \n",
        "\n",
        "      all_graphs.append(Graph(x=x, a=a, y=y)) \n",
        "\n",
        "    # We must return a list of Graph objects \n",
        "    return [all_graphs[i] for i in range(self.n_samples)] "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tB2OOzXplnma"
      },
      "source": [
        "10 MeV:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kLI4jruUdIk0"
      },
      "source": [
        "class MyMonoEnergeticDataset10(Dataset):\n",
        "  def __init__(self, n_samples, **kwargs):\n",
        "    self.n_samples = n_samples\n",
        "    \n",
        "    super().__init__(**kwargs)\n",
        "\n",
        "  def read(self):\n",
        "    all_graphs = []\n",
        "    for i in range(len(D_mono_output10)): \n",
        "      # Node features \n",
        "      x = np.array(D_mono_input10.loc[i]).reshape(96,1) \n",
        "\n",
        "      # Edges \n",
        "      a = adj \n",
        "\n",
        "      # Labels \n",
        "      y = np.zeros(1) \n",
        "      y[0] = D_mono_output10[0][i] \n",
        "\n",
        "      all_graphs.append(Graph(x=x, a=a, y=y)) \n",
        "\n",
        "    # We must return a list of Graph objects \n",
        "    return [all_graphs[i] for i in range(self.n_samples)] "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SfGZoWVdlp2r"
      },
      "source": [
        "20 MeV:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "itHSgsLidIgm"
      },
      "source": [
        "class MyMonoEnergeticDataset20(Dataset):\n",
        "  def __init__(self, n_samples, **kwargs):\n",
        "    self.n_samples = n_samples\n",
        "    \n",
        "    super().__init__(**kwargs)\n",
        "\n",
        "  def read(self):\n",
        "    all_graphs = []\n",
        "    for i in range(len(D_mono_output20)): \n",
        "      # Node features \n",
        "      x = np.array(D_mono_input20.loc[i]).reshape(96,1) \n",
        "\n",
        "      # Edges \n",
        "      a = adj \n",
        "\n",
        "      # Labels \n",
        "      y = np.zeros(1) \n",
        "      y[0] = D_mono_output20[0][i] \n",
        "\n",
        "      all_graphs.append(Graph(x=x, a=a, y=y)) \n",
        "\n",
        "    # We must return a list of Graph objects \n",
        "    return [all_graphs[i] for i in range(self.n_samples)] "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PkbJB237lsOR"
      },
      "source": [
        "30 MeV:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yjJn3rANdLH4"
      },
      "source": [
        "class MyMonoEnergeticDataset30(Dataset):\n",
        "  def __init__(self, n_samples, **kwargs):\n",
        "    self.n_samples = n_samples\n",
        "    \n",
        "    super().__init__(**kwargs)\n",
        "\n",
        "  def read(self):\n",
        "    all_graphs = []\n",
        "    for i in range(len(D_mono_output30)): \n",
        "      # Node features \n",
        "      x = np.array(D_mono_input30.loc[i]).reshape(96,1) \n",
        "\n",
        "      # Edges \n",
        "      a = adj \n",
        "\n",
        "      # Labels \n",
        "      y = np.zeros(1) \n",
        "      y[0] = D_mono_output30[0][i] \n",
        "\n",
        "      all_graphs.append(Graph(x=x, a=a, y=y)) \n",
        "\n",
        "    # We must return a list of Graph objects \n",
        "    return [all_graphs[i] for i in range(self.n_samples)] "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HoLvbOsNluLZ"
      },
      "source": [
        "40 MeV:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5Z6sJ9SdLAZ"
      },
      "source": [
        "class MyMonoEnergeticDataset40(Dataset):\n",
        "  def __init__(self, n_samples, **kwargs):\n",
        "    self.n_samples = n_samples\n",
        "    \n",
        "    super().__init__(**kwargs)\n",
        "\n",
        "  def read(self):\n",
        "    all_graphs = []\n",
        "    for i in range(len(D_mono_output40)): \n",
        "      # Node features \n",
        "      x = np.array(D_mono_input40.loc[i]).reshape(96,1) \n",
        "\n",
        "      # Edges \n",
        "      a = adj \n",
        "\n",
        "      # Labels \n",
        "      y = np.zeros(1) \n",
        "      y[0] = D_mono_output40[0][i] \n",
        "\n",
        "      all_graphs.append(Graph(x=x, a=a, y=y)) \n",
        "\n",
        "    # We must return a list of Graph objects \n",
        "    return [all_graphs[i] for i in range(self.n_samples)] "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OuAUH2-Mr6XD"
      },
      "source": [
        "## Creating dataset:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJ_RNjdVmE1l"
      },
      "source": [
        "Training data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pxVLYfssFnBu"
      },
      "source": [
        "data = MyDataset(max_no_of_graphs_training)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BTsBIcLQFm_q",
        "outputId": "900e928b-d4ee-4e1a-d58e-5ca7f44f1d65"
      },
      "source": [
        "data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MyDataset(n_graphs=173811)"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mVM2zMV3mtwK"
      },
      "source": [
        "idxs = np.random.permutation(len(data))\n",
        "loader_tr = DisjointLoader(data, batch_size=batch_size, epochs=epochs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0f4j8_7DHUEK"
      },
      "source": [
        "# Defining train step:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXakJq2VHTye"
      },
      "source": [
        "def train_step(inputs, target):\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions = model(inputs, training=True)\n",
        "        loss = loss_fn(target, predictions) + sum(model.losses)\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "    \n",
        "    return loss, predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QgMirSEOHjEi"
      },
      "source": [
        "# Model definition:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S8vhGN7KHTwG"
      },
      "source": [
        "class Net(Model):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = GCSConv(32, activation=\"relu\")\n",
        "        self.pool1 = TopKPool(ratio=0.5)\n",
        "        self.conv2 = GCSConv(32, activation=\"relu\")\n",
        "        self.pool2 = TopKPool(ratio=0.5)\n",
        "        self.conv3 = GCSConv(32, activation=\"relu\")\n",
        "        self.global_pool = GlobalAvgPool()\n",
        "        self.dense = Dense(data.n_labels, activation=\"linear\")\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x, a, i = inputs\n",
        "        x = self.conv1([x, a])\n",
        "        x1, a1, i1 = self.pool1([x, a, i])\n",
        "        x1 = self.conv2([x1, a1])\n",
        "        x2, a2, i2 = self.pool1([x1, a1, i1])\n",
        "        x2 = self.conv3([x2, a2])\n",
        "        output = self.global_pool([x2, i2])\n",
        "        output = self.dense(output)\n",
        "\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FPXcz2-bHn-1",
        "outputId": "d7c62624-4e92-4722-ce37-88577a6a0f6c"
      },
      "source": [
        "model = Net()\n",
        "optimizer = Adam(lr=learning_rate)\n",
        "loss_fn = MeanAbsoluteError()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eh0-8K1IHn8t"
      },
      "source": [
        "epoch = step = 0\n",
        "best_val_loss = np.inf\n",
        "best_weights = None\n",
        "patience = es_patience\n",
        "results = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLouho2OItyb"
      },
      "source": [
        "#Training:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vpxPMgihHn6b"
      },
      "source": [
        "combined_predictions = []\n",
        "for batch in loader_tr:\n",
        "    step += 1\n",
        "    loss, prediction = train_step(*batch)\n",
        "    combined_predictions.append(prediction)\n",
        "    results.append((loss))\n",
        "    if step == loader_tr.steps_per_epoch:\n",
        "        step = 0\n",
        "        epoch += 1\n",
        "\n",
        "        # Compute validation loss and accuracy\n",
        "        #val_loss, val_acc = evaluate(loader_va)\n",
        "        print(\n",
        "            \"Epoch: {}, Loss: {:.3f}\".format(\n",
        "                epoch, loss\n",
        "            )\n",
        "        )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_D7KHeNoomS"
      },
      "source": [
        "32 graphs predictions at a time:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Z9H_Rq5o9n2"
      },
      "source": [
        "No of entries in combined predictions = ceil(no. of graphs / batch size)*epoch size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1IK1s76NpJvp"
      },
      "source": [
        "I need the final ceil(no. of graphs / batch size) indices from the combined_predictions list to get the final training output:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3CyoeFVpRHq"
      },
      "source": [
        "len(combined_predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "scoJ718Gtmjq"
      },
      "source": [
        "combined_predictions[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0D2Gxb8o6FX"
      },
      "source": [
        "# Saving the model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78ltpunAK6qR"
      },
      "source": [
        "model.save('/content/drive/MyDrive/SKKU_JSNS^2/GNNs/model2', save_format=\"tf\") "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VwwvfDn0Q-0y"
      },
      "source": [
        "model.save_weights('/content/drive/MyDrive/SKKU_JSNS^2/GNNs/model2_weights')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2KgBPvX1o_kn"
      },
      "source": [
        "# Loading the model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ryc3kb3lRKZn"
      },
      "source": [
        "model.load_weights('/content/drive/MyDrive/SKKU_JSNS^2/GNNs/model2_weights/model2_weights')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EGexzhQzqs6d"
      },
      "source": [
        "#model.load('/content/drive/MyDrive/SKKU_JSNS^2/GNNs/model2')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eN-IeHM-TmN4"
      },
      "source": [
        "# Making predictions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QPTbg_1f37OC"
      },
      "source": [
        "max_no_of_graphs_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9MDc3V8qCEl"
      },
      "source": [
        "No of entries in combined predictions = ceil(no. of graphs / batch size)*epoch size"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R9EnoKmATu2o"
      },
      "source": [
        "data_test = MyTestDataset(math.floor(max_no_of_graphs_test/32) * 32) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96naTXOaqnjq"
      },
      "source": [
        "Taking only the 19296 graphs, for neatness:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17Uw1MU1oa9J"
      },
      "source": [
        "data_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQV7TrD1K284"
      },
      "source": [
        "loader_test = DisjointLoader(data_test, batch_size=32, epochs=1) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ew026QFhq676"
      },
      "source": [
        "Predictions are made by training the model on the test graphs, and then reload the original model saved, so as to forget the weights trained using the test data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJ6bwkDMRmUR"
      },
      "source": [
        "combined_predictions = []\n",
        "loss_combined = []\n",
        "epoch_count = 0\n",
        "for batch in loader_test:\n",
        "  loss, prediction = train_step(*batch)\n",
        "  combined_predictions.append(prediction)\n",
        "  loss_combined.append(loss)\n",
        "  if (len(combined_predictions)% math.floor(max_no_of_graphs_test/32)) == 0:\n",
        "    epoch_count = epoch_count + 1\n",
        "    #print(epoch_count)\n",
        "    print(sum(loss_combined)/len(loss_combined))\n",
        "    model.load_weights('/content/drive/MyDrive/SKKU_JSNS^2/GNNs/model2_weights/model2_weights') #reload original model\n",
        "    #if not ((epoch_count == 9) or (epoch_count == 10)):\n",
        "    #  combined_predictions = []\n",
        "  #predictions = model(batch, training=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "niNrpY-oX0JT"
      },
      "source": [
        "mae = float(np.array(sum(loss_combined)/len(loss_combined)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OIzOc-7nSeGS"
      },
      "source": [
        "len(combined_predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ht6uiYvar2lj"
      },
      "source": [
        "Calculating number of graphs predicted on:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-3Tx2tDXUey"
      },
      "source": [
        "n_graphs = math.floor(max_no_of_graphs_test/32) * 32"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLqloO_m3v_X"
      },
      "source": [
        "n_graphs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6LuSOFKVoWS"
      },
      "source": [
        "final_predictions = np.zeros(n_graphs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "km44vjJCUXVw"
      },
      "source": [
        "for i in range(len(combined_predictions)): \n",
        "  a = np.array(combined_predictions[i]).reshape(32) \n",
        "  for j in range(32):\n",
        "    final_predictions[(32*i) + j] = a[j] "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3OljoOwIWeO4"
      },
      "source": [
        "final_predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rn1bxW39sb71"
      },
      "source": [
        "# Getting truth values from the shuffled graphs:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97e2DwaMsn9R"
      },
      "source": [
        "DF_graphs_test_y = pd.DataFrame(np.zeros(n_graphs))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-nvpEb8agd3"
      },
      "source": [
        "for i in range(len(DF_graphs_test_y)):\n",
        "  DF_graphs_test_y[0][i] = data_test[i].y[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMR9pmSfXFnC"
      },
      "source": [
        "# Correlation plot:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbG890NcibtV"
      },
      "source": [
        "fig = plt.figure(figsize=(10, 10))\n",
        "D_pred = pd.DataFrame(final_predictions)\n",
        "plt.hexbin(DF_graphs_test_y[0], D_pred[0], bins = 301, mincnt = 0.1)\n",
        "cbar = plt.colorbar()\n",
        "\n",
        "cbar.ax.tick_params(labelsize=25)\n",
        "cbar.set_label('# of events',  fontsize = 20)\n",
        "params = {'mathtext.default': 'regular' }          \n",
        "plt.rcParams.update(params)\n",
        "#plt.plot(range(60), range(60), color = 'red')\n",
        "plt.xticks(fontsize=20)\n",
        "plt.yticks(fontsize=20)\n",
        "plt.xlabel('$E_{MC} $(MeV)', fontsize = 20)\n",
        "plt.ylabel('$E_{pred}$ (MeV)', fontsize = 20)\n",
        "plt.title(label = 'GNN architecture \\n Optimization technique: Manual\\n MAE = ' + str(round(mae,4)) + '\\n \\n $E_{pred}$ vs $E_{MC} $' , fontsize = 10)\n",
        "#plt.savefig(\"/content/drive/MyDrive/SKKU_JSNS^2/Plots/GNNs/correlation.jpeg\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdQ2URoZk8Lk"
      },
      "source": [
        "# Loading mono-energetic data:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WlVXneIimssI"
      },
      "source": [
        "Reminder: 32 is the batch size"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53CZ7pH6k1sT"
      },
      "source": [
        "#Loading mono-energetic data in the form of graphs\n",
        "n_graphs1 = math.floor(len(D_mono_input1)/32) * 32\n",
        "data_test1 = MyMonoEnergeticDataset1(n_graphs1)\n",
        "n_graphs5 = math.floor(len(D_mono_input5)/32) * 32\n",
        "data_test5 = MyMonoEnergeticDataset5(n_graphs5)\n",
        "n_graphs8 = math.floor(len(D_mono_input8)/32) * 32\n",
        "data_test8 = MyMonoEnergeticDataset8(n_graphs8)\n",
        "n_graphs10 = math.floor(len(D_mono_input10)/32) * 32\n",
        "data_test10 = MyMonoEnergeticDataset10(n_graphs10)\n",
        "n_graphs20 = math.floor(len(D_mono_input20)/32) * 32\n",
        "data_test20 = MyMonoEnergeticDataset20(n_graphs20)\n",
        "n_graphs30 = math.floor(len(D_mono_input30)/32) * 32\n",
        "data_test30 = MyMonoEnergeticDataset30(n_graphs30)\n",
        "n_graphs40 = math.floor(len(D_mono_input40)/32) * 32\n",
        "data_test40 = MyMonoEnergeticDataset40(n_graphs40)\n",
        "\n",
        "#loading mono-energetic test data to the DisjointLoader\n",
        "loader_test1 = DisjointLoader(data_test1, batch_size=32, epochs=1) \n",
        "loader_test5 = DisjointLoader(data_test5, batch_size=32, epochs=1) \n",
        "loader_test8 = DisjointLoader(data_test8, batch_size=32, epochs=1) \n",
        "loader_test10 = DisjointLoader(data_test10, batch_size=32, epochs=1) \n",
        "loader_test20 = DisjointLoader(data_test20, batch_size=32, epochs=1) \n",
        "loader_test30 = DisjointLoader(data_test30, batch_size=32, epochs=1) \n",
        "loader_test40 = DisjointLoader(data_test40, batch_size=32, epochs=1) \n",
        "\n",
        "#empty arrays to store the final mono-energetic predictions\n",
        "final_predictions1 = np.zeros(n_graphs1)\n",
        "final_predictions5 = np.zeros(n_graphs5)\n",
        "final_predictions8 = np.zeros(n_graphs8)\n",
        "final_predictions10 = np.zeros(n_graphs10)\n",
        "final_predictions20 = np.zeros(n_graphs20)\n",
        "final_predictions30 = np.zeros(n_graphs30)\n",
        "final_predictions40 = np.zeros(n_graphs40)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UAdLLeQhlIU9"
      },
      "source": [
        "Making Predictions on the Mono-energetic data graphs:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f780fqUppktx"
      },
      "source": [
        "1 MeV:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LTOy5AISlT-P"
      },
      "source": [
        "#Lines with hashtags need change of variables:\n",
        "\n",
        "model.load_weights('/content/drive/MyDrive/SKKU_JSNS^2/GNNs/model2_weights/model2_weights')\n",
        "combined_predictions1 = [] ##\n",
        "loss_combined1 = [] #\n",
        "epoch_count = 0\n",
        "for batch in loader_test1: #\n",
        "  loss, prediction = train_step(*batch) \n",
        "  combined_predictions1.append(prediction) #\n",
        "  loss_combined1.append(loss) #\n",
        "  if (len(combined_predictions1)% math.floor(len(D_mono_input1)/32)) == 0: #\n",
        "    epoch_count = epoch_count + 1\n",
        "    #print(epoch_count)\n",
        "    #print(sum(loss_combined)/len(loss_combined))\n",
        "    model.load_weights('/content/drive/MyDrive/SKKU_JSNS^2/GNNs/model2_weights/model2_weights')\n",
        "\n",
        "for i in range(len(combined_predictions1)):  #\n",
        "  a = np.array(combined_predictions1[i]).reshape(32) #\n",
        "  for j in range(32):\n",
        "    final_predictions1[(32*i) + j] = a[j] #\n",
        "\n",
        "D_pred1 = pd.DataFrame(final_predictions1) #\n",
        "\n",
        "DF_graphs_test_y1 = pd.DataFrame(np.zeros(n_graphs1)) #\n",
        "\n",
        "for i in range(len(DF_graphs_test_y1)): #\n",
        "  DF_graphs_test_y1[0][i] = data_test1[i].y[0] #"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WSXKjywzpmwR"
      },
      "source": [
        "5 MeV:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4JkuMuUppmix"
      },
      "source": [
        "#Lines with hashtags need change of variables:\n",
        "\n",
        "model.load_weights('/content/drive/MyDrive/SKKU_JSNS^2/GNNs/model2_weights/model2_weights')\n",
        "combined_predictions5 = [] ##\n",
        "loss_combined5 = [] #\n",
        "epoch_count = 0\n",
        "for batch in loader_test5: #\n",
        "  loss, prediction = train_step(*batch) \n",
        "  combined_predictions5.append(prediction) #\n",
        "  loss_combined5.append(loss) #\n",
        "  if (len(combined_predictions5)% math.floor(len(D_mono_input5)/32)) == 0: #\n",
        "    epoch_count = epoch_count + 1\n",
        "    #print(epoch_count)\n",
        "    #print(sum(loss_combined)/len(loss_combined))\n",
        "    model.load_weights('/content/drive/MyDrive/SKKU_JSNS^2/GNNs/model2_weights/model2_weights')\n",
        "\n",
        "for i in range(len(combined_predictions5)):  #\n",
        "  a = np.array(combined_predictions5[i]).reshape(32) #\n",
        "  for j in range(32):\n",
        "    final_predictions5[(32*i) + j] = a[j] #\n",
        "\n",
        "D_pred5 = pd.DataFrame(final_predictions5) #\n",
        "\n",
        "DF_graphs_test_y5 = pd.DataFrame(np.zeros(n_graphs5)) #\n",
        "\n",
        "for i in range(len(DF_graphs_test_y5)): #\n",
        "  DF_graphs_test_y5[0][i] = data_test5[i].y[0] #"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Mf6odTBpoF-"
      },
      "source": [
        "8 MeV:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vDwg3pTspmg0"
      },
      "source": [
        "#Lines with hashtags need change of variables:\n",
        "\n",
        "model.load_weights('/content/drive/MyDrive/SKKU_JSNS^2/GNNs/model2_weights/model2_weights')\n",
        "combined_predictions8 = [] ##\n",
        "loss_combined8 = [] #\n",
        "epoch_count = 0\n",
        "for batch in loader_test8: #\n",
        "  loss, prediction = train_step(*batch) \n",
        "  combined_predictions8.append(prediction) #\n",
        "  loss_combined8.append(loss) #\n",
        "  if (len(combined_predictions8)% math.floor(len(D_mono_input8)/32)) == 0: #\n",
        "    epoch_count = epoch_count + 1\n",
        "    #print(epoch_count)\n",
        "    #print(sum(loss_combined)/len(loss_combined))\n",
        "    model.load_weights('/content/drive/MyDrive/SKKU_JSNS^2/GNNs/model2_weights/model2_weights')\n",
        "\n",
        "for i in range(len(combined_predictions8)):  #\n",
        "  a = np.array(combined_predictions8[i]).reshape(32) #\n",
        "  for j in range(32):\n",
        "    final_predictions8[(32*i) + j] = a[j] #\n",
        "\n",
        "D_pred8 = pd.DataFrame(final_predictions8) #\n",
        "\n",
        "DF_graphs_test_y8 = pd.DataFrame(np.zeros(n_graphs8)) #\n",
        "\n",
        "for i in range(len(DF_graphs_test_y8)): #\n",
        "  DF_graphs_test_y8[0][i] = data_test8[i].y[0] #"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6yw3QzzpwjI"
      },
      "source": [
        "10 MeV:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3SC0c_kpmeR"
      },
      "source": [
        "#Lines with hashtags need change of variables:\n",
        "\n",
        "model.load_weights('/content/drive/MyDrive/SKKU_JSNS^2/GNNs/model2_weights/model2_weights')\n",
        "combined_predictions10 = [] ##\n",
        "loss_combined10 = [] #\n",
        "epoch_count = 0\n",
        "for batch in loader_test10: #\n",
        "  loss, prediction = train_step(*batch) \n",
        "  combined_predictions10.append(prediction) #\n",
        "  loss_combined10.append(loss) #\n",
        "  if (len(combined_predictions10)% math.floor(len(D_mono_input10)/32)) == 0: #\n",
        "    epoch_count = epoch_count + 1\n",
        "    #print(epoch_count)\n",
        "    #print(sum(loss_combined)/len(loss_combined))\n",
        "    model.load_weights('/content/drive/MyDrive/SKKU_JSNS^2/GNNs/model2_weights/model2_weights')\n",
        "\n",
        "for i in range(len(combined_predictions10)):  #\n",
        "  a = np.array(combined_predictions10[i]).reshape(32) #\n",
        "  for j in range(32):\n",
        "    final_predictions10[(32*i) + j] = a[j] #\n",
        "\n",
        "D_pred10 = pd.DataFrame(final_predictions10) #\n",
        "\n",
        "DF_graphs_test_y10 = pd.DataFrame(np.zeros(n_graphs10)) #\n",
        "\n",
        "for i in range(len(DF_graphs_test_y10)): #\n",
        "  DF_graphs_test_y10[0][i] = data_test10[i].y[0] #"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-h4Yht_px8f"
      },
      "source": [
        "20 MeV:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y31xQ-LdpmcF"
      },
      "source": [
        "#Lines with hashtags need change of variables:\n",
        "\n",
        "model.load_weights('/content/drive/MyDrive/SKKU_JSNS^2/GNNs/model2_weights/model2_weights')\n",
        "combined_predictions20 = [] ##\n",
        "loss_combined20 = [] #\n",
        "epoch_count = 0\n",
        "for batch in loader_test20: #\n",
        "  loss, prediction = train_step(*batch) \n",
        "  combined_predictions20.append(prediction) #\n",
        "  loss_combined20.append(loss) #\n",
        "  if (len(combined_predictions20)% math.floor(len(D_mono_input20)/32)) == 0: #\n",
        "    epoch_count = epoch_count + 1\n",
        "    #print(epoch_count)\n",
        "    #print(sum(loss_combined)/len(loss_combined))\n",
        "    model.load_weights('/content/drive/MyDrive/SKKU_JSNS^2/GNNs/model2_weights/model2_weights')\n",
        "\n",
        "for i in range(len(combined_predictions20)):  #\n",
        "  a = np.array(combined_predictions20[i]).reshape(32) #\n",
        "  for j in range(32):\n",
        "    final_predictions20[(32*i) + j] = a[j] #\n",
        "\n",
        "D_pred20 = pd.DataFrame(final_predictions20) #\n",
        "\n",
        "DF_graphs_test_y20 = pd.DataFrame(np.zeros(n_graphs20)) #\n",
        "\n",
        "for i in range(len(DF_graphs_test_y20)): #\n",
        "  DF_graphs_test_y20[0][i] = data_test20[i].y[0] #"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63yZvByrpzFr"
      },
      "source": [
        "30 MeV:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_4o58Y7pmX0"
      },
      "source": [
        "#Lines with hashtags need change of variables:\n",
        "\n",
        "model.load_weights('/content/drive/MyDrive/SKKU_JSNS^2/GNNs/model2_weights/model2_weights')\n",
        "combined_predictions30 = [] ##\n",
        "loss_combined30 = [] #\n",
        "epoch_count = 0\n",
        "for batch in loader_test30: #\n",
        "  loss, prediction = train_step(*batch) \n",
        "  combined_predictions30.append(prediction) #\n",
        "  loss_combined30.append(loss) #\n",
        "  if (len(combined_predictions30)% math.floor(len(D_mono_input30)/32)) == 0: #\n",
        "    epoch_count = epoch_count + 1\n",
        "    #print(epoch_count)\n",
        "    #print(sum(loss_combined)/len(loss_combined))\n",
        "    model.load_weights('/content/drive/MyDrive/SKKU_JSNS^2/GNNs/model2_weights/model2_weights')\n",
        "\n",
        "for i in range(len(combined_predictions30)):  #\n",
        "  a = np.array(combined_predictions30[i]).reshape(32) #\n",
        "  for j in range(32):\n",
        "    final_predictions30[(32*i) + j] = a[j] #\n",
        "\n",
        "D_pred30 = pd.DataFrame(final_predictions30) #\n",
        "\n",
        "DF_graphs_test_y30 = pd.DataFrame(np.zeros(n_graphs30)) #\n",
        "\n",
        "for i in range(len(DF_graphs_test_y30)): #\n",
        "  DF_graphs_test_y30[0][i] = data_test30[i].y[0] #"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wha1cnYnp0WS"
      },
      "source": [
        "40 MeV:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uw4OcTRHpmVk"
      },
      "source": [
        "#Lines with hashtags need change of variables:\n",
        "\n",
        "model.load_weights('/content/drive/MyDrive/SKKU_JSNS^2/GNNs/model2_weights/model2_weights')\n",
        "combined_predictions40 = [] ##\n",
        "loss_combined40 = [] #\n",
        "epoch_count = 0\n",
        "for batch in loader_test40: #\n",
        "  loss, prediction = train_step(*batch) \n",
        "  combined_predictions40.append(prediction) #\n",
        "  loss_combined40.append(loss) #\n",
        "  if (len(combined_predictions40)% math.floor(len(D_mono_input40)/32)) == 0: #\n",
        "    epoch_count = epoch_count + 1\n",
        "    #print(epoch_count)\n",
        "    #print(sum(loss_combined)/len(loss_combined))\n",
        "    model.load_weights('/content/drive/MyDrive/SKKU_JSNS^2/GNNs/model2_weights/model2_weights')\n",
        "\n",
        "for i in range(len(combined_predictions40)):  #\n",
        "  a = np.array(combined_predictions40[i]).reshape(32) #\n",
        "  for j in range(32):\n",
        "    final_predictions40[(32*i) + j] = a[j] #\n",
        "\n",
        "D_pred40 = pd.DataFrame(final_predictions40) #\n",
        "\n",
        "DF_graphs_test_y40 = pd.DataFrame(np.zeros(n_graphs40)) #\n",
        "\n",
        "for i in range(len(DF_graphs_test_y40)): #\n",
        "  DF_graphs_test_y40[0][i] = data_test40[i].y[0] #"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QI-oq0TAuVxL"
      },
      "source": [
        "# Saving predictions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dtWW-oRr5JQJ"
      },
      "source": [
        "np.save('/content/drive/MyDrive/SKKU_JSNS^2/Mono_energetic_predictions/GNNs/1MeV', final_predictions1)\n",
        "np.save('/content/drive/MyDrive/SKKU_JSNS^2/Mono_energetic_predictions/GNNs/5MeV', final_predictions5)\n",
        "np.save('/content/drive/MyDrive/SKKU_JSNS^2/Mono_energetic_predictions/GNNs/8MeV', final_predictions8)\n",
        "np.save('/content/drive/MyDrive/SKKU_JSNS^2/Mono_energetic_predictions/GNNs/10MeV', final_predictions10)\n",
        "np.save('/content/drive/MyDrive/SKKU_JSNS^2/Mono_energetic_predictions/GNNs/20MeV', final_predictions20)\n",
        "np.save('/content/drive/MyDrive/SKKU_JSNS^2/Mono_energetic_predictions/GNNs/30MeV', final_predictions30)\n",
        "np.save('/content/drive/MyDrive/SKKU_JSNS^2/Mono_energetic_predictions/GNNs/40MeV', final_predictions40)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}